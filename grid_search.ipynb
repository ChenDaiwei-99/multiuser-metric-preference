{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# customized packages\n",
    "import dataset\n",
    "import utils\n",
    "from mlp import projector\n",
    "from metricpref_learner import MetricPrefLearner\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### color dataset ################\n",
    "N = 48\n",
    "color_dataset = dataset.Dataset(dataset_type='Color',color_path='./CPdata.mat', N=N)\n",
    "color_data = color_dataset.getAllData()\n",
    "items, observations, true_y, true_M, true_u = color_data['X'], color_data['S'], color_data['Y'], color_data['M'], color_data['U']\n",
    "# automatically extract 300 samples from each user\n",
    "obs_train, obs_test, Y_train, Y_test = color_dataset.getTrainTestSplit(train_size=48*300)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### hyperparameters ################\n",
    "epochs = 2000\n",
    "bs = 32\n",
    "lr = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 0.005\n",
    "dim = 3\n",
    "device = 'mps'\n",
    "\n",
    "# TODO: design different weight decay rate for us and L\n",
    "weight_decay_us = 0\n",
    "weight_decay_net = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ define dataloader ################\n",
    "class CustomDataset(Dataset):   # written by the gpt-4 :)\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_dataset = CustomDataset(list(zip(obs_train, Y_train)))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(list(zip(obs_test, Y_test)))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ initialize the model and the learner################\n",
    "net = projector(feature_dim=dim, num_layer=1, num_class=dim, bias_ind=False)\n",
    "learner = MetricPrefLearner(dim_feature=dim, num_users=N, items=items)\n",
    "learner.assignModel(net)\n",
    "\n",
    "us_params = []\n",
    "net_params = []\n",
    "for name, param in learner.named_parameters():\n",
    "    if 'us' in name:\n",
    "        us_params.append(param)\n",
    "    else:\n",
    "        net_params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ define loss and optimizer ################\n",
    "def hinge_loss(outputs, targets):   # written by the gpt-4 :)\n",
    "    hinge_loss_value = 1 - targets * outputs\n",
    "    hinge_loss_value = torch.clamp(hinge_loss_value, min=0)\n",
    "    return hinge_loss_value.mean()\n",
    "\n",
    "def logistic(z):\n",
    "    return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "def nll_logistic(predictions, targets):\n",
    "    # Ensure targets are -1 or 1\n",
    "    assert torch.all((targets == -1) | (targets == 1))\n",
    "    \n",
    "    probabilities = logistic(targets * predictions)\n",
    "    # print(probabilities)\n",
    "    nll = -torch.sum(torch.log(probabilities))\n",
    "    # print(nll)\n",
    "    return nll\n",
    "\n",
    "weight_decay_dic = [\n",
    "    {'params': us_params, 'weight_decay': weight_decay_us},\n",
    "    {'params': net_params, 'weight_decay': weight_decay_net}\n",
    "]\n",
    "\n",
    "\n",
    "loss_fn = hinge_loss\n",
    "optimizer = torch.optim.Adam(weight_decay_dic, lr=1e-3, betas=(0.9, 0.999), eps=1e-8)\n",
    "# optimizer = torch.optim.SGD(learner.parameters(),lr=lr,momentum=momentum,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learner,optimizer,loss_fn,train_loader,test_loader,epochs,relative_error_ind=True):\n",
    "    train_stats = {\n",
    "        'train_loss_per_batch_record': [],\n",
    "        'train_accu_record': [],\n",
    "        'test_loss_per_batch_record': [],\n",
    "        'test_accu_record': [],\n",
    "        'relative_metric_error_record': [],\n",
    "        'relative_ideal_point_error_record': [],\n",
    "    }\n",
    "\n",
    "    tqdmr = tqdm(range(epochs))\n",
    "    for ep in tqdmr:\n",
    "        for x,y in train_loader:\n",
    "            # x[0] = x[0].to(device)\n",
    "            # x[1][0] = x[1][0].to(device)\n",
    "            # x[1][1] = x[1][1].to(device)\n",
    "            # y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_delta = learner(x)\n",
    "            acc_batch = torch.sum((pred_delta * y)>0)\n",
    "            if relative_error_ind:\n",
    "                # calculate the relative_metric_error\n",
    "                L = net.layers.weight\n",
    "                relative_metric_error = torch.norm(L.T @ L - torch.tensor(true_M)) / torch.norm(torch.tensor(true_M))\n",
    "                train_stats['relative_metric_error_record'].append(relative_metric_error.item())\n",
    "                # calculate the relative_ideal_point_error\n",
    "                relative_ideal_point_error = (torch.norm(torch.tensor(true_u)-learner.us) / torch.norm(torch.tensor(true_u))).item()\n",
    "                train_stats['relative_ideal_point_error_record'].append(relative_ideal_point_error)\n",
    "            loss = loss_fn(pred_delta,y)\n",
    "            train_stats['train_loss_per_batch_record'].append(loss.item())\n",
    "            train_stats['train_accu_record'].append(acc_batch/bs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_stat = val(net,loss_fn,test_loader)\n",
    "        train_stats['test_loss_per_batch_record'].extend(val_stat['test_loss_per_batch'])\n",
    "        train_stats['test_accu_record'].append(val_stat['test_accu'])\n",
    "        tqdmr.set_postfix({'test_accu': val_stat['test_accu']})\n",
    "    return train_stats\n",
    "\n",
    "def val(net,loss_fn,test_loader):\n",
    "    total_val_samples = len(test_dataloader.dataset)\n",
    "    val_stat = {\n",
    "        'test_correct': 0,\n",
    "        'test_loss_per_batch': [],\n",
    "        'test_accu': 0\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            # x[0] = x[0].to(device)\n",
    "            # x[1][0] = x[1][0].to(device)\n",
    "            # x[1][1] = x[1][1].to(device)\n",
    "            # y = y.to(device)\n",
    "            pred_delta = learner(x)\n",
    "            acc_batch = torch.sum((pred_delta * y)>0)\n",
    "            loss = loss_fn(pred_delta,y)\n",
    "\n",
    "            val_stat['test_correct'] += acc_batch.item()\n",
    "            val_stat['test_loss_per_batch'].append(loss.item())\n",
    "    \n",
    "    val_stat['test_accu'] = val_stat['test_correct'] / total_val_samples\n",
    "\n",
    "    return val_stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train(learner, optimizer, loss_fn, train_dataloader, test_dataloader, epochs, relative_error_ind=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,3,figsize=(20,10))\n",
    "ax_flatten = ax.flatten()\n",
    "for idx,key in enumerate(train_stats):\n",
    "    ax_flatten[idx].plot(train_stats[key])\n",
    "    ax_flatten[idx].set_ylabel(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learner,optimizer,loss_fn,train_loader,test_loader,epochs,relative_error_ind=True):\n",
    "    train_stats = {\n",
    "        'train_loss_per_batch_record': [],\n",
    "        'train_accu_record': [],\n",
    "        'test_loss_per_batch_record': [],\n",
    "        'test_accu_record': [],\n",
    "        'relative_metric_error_record': [],\n",
    "        'relative_ideal_point_error_record': [],\n",
    "    }\n",
    "\n",
    "    tqdmr = tqdm(range(epochs))\n",
    "    for ep in tqdmr:\n",
    "        for x,y in train_loader:\n",
    "            # x[0] = x[0].to(device)\n",
    "            # x[1][0] = x[1][0].to(device)\n",
    "            # x[1][1] = x[1][1].to(device)\n",
    "            # y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_delta = learner(x)\n",
    "            acc_batch = torch.sum((pred_delta * y)>0)\n",
    "            if relative_error_ind:\n",
    "                # calculate the relative_metric_error\n",
    "                L = net.layers.weight\n",
    "                relative_metric_error = torch.norm(L.T @ L - torch.tensor(true_M)) / torch.norm(torch.tensor(true_M))\n",
    "                train_stats['relative_metric_error_record'].append(relative_metric_error.item())\n",
    "                # calculate the relative_ideal_point_error\n",
    "                relative_ideal_point_error = (torch.norm(torch.tensor(true_u)-learner.us) / torch.norm(torch.tensor(true_u))).item()\n",
    "                train_stats['relative_ideal_point_error_record'].append(relative_ideal_point_error)\n",
    "            loss = loss_fn(pred_delta,y)\n",
    "            train_stats['train_loss_per_batch_record'].append(loss.item())\n",
    "            train_stats['train_accu_record'].append(acc_batch/len(y))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        val_stat = val(learner,loss_fn,test_loader)\n",
    "        train_stats['test_loss_per_batch_record'].extend(val_stat['test_loss_per_batch'])\n",
    "        train_stats['test_accu_record'].append(val_stat['test_accu'])\n",
    "        tqdmr.set_postfix({'test_accu': val_stat['test_accu']})\n",
    "    return train_stats\n",
    "\n",
    "def val(learner,loss_fn,test_loader):\n",
    "    total_val_samples = len(test_loader.dataset)\n",
    "    val_stat = {\n",
    "        'test_correct': 0,\n",
    "        'test_loss_per_batch': [],\n",
    "        'test_accu': 0\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            # x[0] = x[0].to(device)\n",
    "            # x[1][0] = x[1][0].to(device)\n",
    "            # x[1][1] = x[1][1].to(device)\n",
    "            # y = y.to(device)\n",
    "            pred_delta = learner(x)\n",
    "            acc_batch = torch.sum((pred_delta * y)>0)\n",
    "            loss = loss_fn(pred_delta,y)\n",
    "\n",
    "            val_stat['test_correct'] += acc_batch.item()\n",
    "            val_stat['test_loss_per_batch'].append(loss.item())\n",
    "    \n",
    "    val_stat['test_accu'] = val_stat['test_correct'] / total_val_samples\n",
    "\n",
    "    return val_stat\n",
    "\n",
    "class CustomDataset(Dataset):   # written by the gpt-4 :)\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "    \n",
    "def hinge_loss(outputs, targets):   # written by the gpt-4 :)\n",
    "    hinge_loss_value = 1 - targets * outputs\n",
    "    hinge_loss_value = torch.clamp(hinge_loss_value, min=0)\n",
    "    return hinge_loss_value.mean()\n",
    "\n",
    "def logistic(z):\n",
    "    return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "def nll_logistic(predictions, targets):\n",
    "    # Ensure targets are -1 or 1\n",
    "    assert torch.all((targets == -1) | (targets == 1))\n",
    "    \n",
    "    probabilities = logistic(targets * predictions)\n",
    "    # print(probabilities)\n",
    "    nll = -torch.sum(torch.log(probabilities))\n",
    "    # print(nll)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(args):\n",
    "    ########### hyperparameters from args ################\n",
    "    epochs = args['epochs']\n",
    "    bs = args['bs']\n",
    "    lr = args['lr']\n",
    "    feature_dim = args['feature_dim']\n",
    "    num_users = args['num_users']  # fixed at 48 for color dataset \n",
    "    num_pairs_per_user = args['num_pairs_per_user']\n",
    "    # design different weight decay rate for us and L\n",
    "    weight_decay_us = args['weight_decay_us']\n",
    "    weight_decay_net = args['weight_decay_net']\n",
    "\n",
    "    # ########### hyperparameters ################\n",
    "    # epochs = 2000\n",
    "    # bs = 32\n",
    "    # lr = 1e-3\n",
    "    # momentum = 0.9\n",
    "    # feature_dim = 3\n",
    "    # device = 'mps'\n",
    "    # num_users = 48  # fixed at 48 for color dataset \n",
    "    # num_pairs_per_user = 300\n",
    "\n",
    "    # # design different weight decay rate for us and L\n",
    "    # weight_decay_us = 0\n",
    "    # weight_decay_net = 0\n",
    "\n",
    "    ########### color dataset ################\n",
    "    color_dataset = dataset.Dataset(dataset_type='Color',color_path='./CPdata.mat', N=num_users)\n",
    "    color_data = color_dataset.getAllData()\n",
    "    items, observations, true_y, true_M, true_u = color_data['X'], color_data['S'], color_data['Y'], color_data['M'], color_data['U']\n",
    "    # automatically extract 300 samples from each user\n",
    "    obs_train, obs_test, Y_train, Y_test = color_dataset.getTrainTestSplit(train_size=num_users*num_pairs_per_user)\n",
    "\n",
    "    ############ define dataloader ################\n",
    "    train_dataset = CustomDataset(list(zip(obs_train, Y_train)))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    test_dataset = CustomDataset(list(zip(obs_test, Y_test)))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    ############ initialize the model and the learner################\n",
    "    net = projector(feature_dim=feature_dim, num_layer=1, num_class=feature_dim, bias_ind=False)\n",
    "    learner = MetricPrefLearner(dim_feature=feature_dim, num_users=num_users, items=items)\n",
    "    learner.assignModel(net)\n",
    "    us_params = []\n",
    "    net_params = []\n",
    "    for name, param in learner.named_parameters():\n",
    "        if 'us' in name:\n",
    "            us_params.append(param)\n",
    "        else:\n",
    "            net_params.append(param)\n",
    "\n",
    "    ############ define loss and optimizer ################\n",
    "    weight_decay_dic = [\n",
    "        {'params': us_params, 'weight_decay': weight_decay_us},\n",
    "        {'params': net_params, 'weight_decay': weight_decay_net}\n",
    "    ]\n",
    "\n",
    "    loss_fn = hinge_loss\n",
    "    optimizer = torch.optim.Adam(weight_decay_dic, lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
    "    # optimizer = torch.optim.SGD(learner.parameters(),lr=lr,momentum=momentum,weight_decay=weight_decay)\n",
    "    train_stats = train(learner, optimizer, loss_fn, train_dataloader, test_dataloader, epochs, relative_error_ind=False)\n",
    "    \n",
    "    return train_stats, learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1255/2000 [06:52<04:24,  2.82it/s, test_accu=0.604]"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'epochs': 2000,\n",
    "    'bs': 32,\n",
    "    'lr': 1e-3,\n",
    "    'feature_dim': 3,\n",
    "    'num_users': 48,\n",
    "    'num_pairs_per_user': 300,\n",
    "    'weight_decay_us': 0,\n",
    "    'weight_decay_net': 0,\n",
    "}\n",
    "\n",
    "train_stats = train_main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,3,figsize=(20,10))\n",
    "ax_flatten = ax.flatten()\n",
    "for idx,key in enumerate(train_stats):\n",
    "    ax_flatten[idx].plot(train_stats[key])\n",
    "    ax_flatten[idx].set_ylabel(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "lr_list = [1e-2,5e-3,1e-3,5e-4,1e-4]\n",
    "bs_list = [16,32,64,128]\n",
    "weight_decay_us_list = [1e-3,5e-4,1e-4,0]\n",
    "weight_decay_net_list = [1e-3,5e-4,1e-4,0]\n",
    "\n",
    "param_combinations = list(itertools.product(lr_list, bs_list, weight_decay_us_list, weight_decay_net_list))\n",
    "\n",
    "for lr, bs, weight_decay_us, weight_decay_net in param_combinations:\n",
    "    args = {\n",
    "        'epochs': 2000,\n",
    "        'bs': bs,\n",
    "        'lr': lr,\n",
    "        'feature_dim': 3,\n",
    "        'num_users': 48,\n",
    "        'num_pairs_per_user': 300,\n",
    "        'weight_decay_us': weight_decay_us,\n",
    "        'weight_decay_net': weight_decay_net,\n",
    "    }\n",
    "    print('current params:', args)\n",
    "    train_stats = train_main(args)\n",
    "    torch.save({'args':args, 'train_stats': train_stats},f'grid_search_lr{lr}_bs{bs}_wdu{weight_decay_us}_wdn{weight_decay_net}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
