{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Preference Dataset Generator\n",
    "- original code\n",
    "- subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### hyperparameters for the dataset ################\n",
    "feature_dim = 20\n",
    "metric_rank = 1\n",
    "num_items = 100\n",
    "num_users = 10\n",
    "num_pairs_per_user = 300\n",
    "noise_type = 'logistic'\n",
    "noise_beta = 1\n",
    "# noise_type = 'none'\n",
    "# noise_beta = 0\n",
    "num_groups = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io as sio\n",
    "import utils\n",
    "\n",
    "\n",
    "'''\n",
    "Desc: Generates normally distributed data along with ground-truth metric and user points (Refer to the original code)\n",
    "\n",
    "Inputs:\n",
    "    feature_dim: feature dimensions\n",
    "    metric_rank: metric rank\n",
    "    num_items: number of items\n",
    "    num_users: number of users\n",
    "    num_pairs_per_user: number of measurements per user\n",
    "    noise type: type of noise in measurement model, from {'none', 'logistic'}\n",
    "    noise_beta: noise level\n",
    "    num_groups: number of subgroups\n",
    "'''\n",
    "\n",
    "assert metric_rank <= feature_dim, 'metric rank must be equal to or smaller then feature dim'\n",
    "\n",
    "# generate ground-truth metric\n",
    "if metric_rank < feature_dim:\n",
    "    # low-dimensional orthogonal matrix\n",
    "    L = st.ortho_group.rvs(dim=feature_dim)\n",
    "    L = L[:,:metric_rank]\n",
    "    M = (feature_dim / np.sqrt(metric_rank)) * L @ L.T  # keep the frob_norm = feature_dim\n",
    "elif metric_rank == feature_dim:\n",
    "    # arbitrary PSD metric, normalized to have Frobenius norm of d\n",
    "    L = np.random.multivariate_normal(np.zeros(feature_dim), np.eye(feature_dim), feature_dim)\n",
    "    M = L @ L.T\n",
    "    M = M * (feature_dim / np.linalg.norm(M, 'fro'))\n",
    "\n",
    "# generate user points (methods depend on the size of the num_groups)\n",
    "if num_groups == 1:\n",
    "    # original code\n",
    "    U = np.random.multivariate_normal(np.zeros(feature_dim), (1/feature_dim)*np.eye(feature_dim), num_users).T\n",
    "    # pseudo user points\n",
    "    V = -2*M @ U\n",
    "    # generate items and comparisons\n",
    "    Xdata = np.random.multivariate_normal(np.zeros(feature_dim), (1/feature_dim)*np.eye(feature_dim), num_items).T\n",
    "    S = list(zip(list(np.repeat(list(range(num_users)), num_pairs_per_user)),\n",
    "                       [tuple(np.random.choice(num_items, 2, replace=False)) for _ in range(num_pairs_per_user*num_users)]))\n",
    "    Y, Y_noiseless, Y_unquant = utils.one_bit_pairs(Xdata, S, M, V, noise_type, noise_beta)\n",
    "elif num_groups > 1:\n",
    "    # only need to create several ideal points (num_groups)\n",
    "    U_subgroups = np.random.multivariate_normal(np.zeros(feature_dim), (1/feature_dim)*np.eye(feature_dim), num_groups).T\n",
    "    # Then we need to simulate different users, each user's point is a weight-average of the ideal points\n",
    "    alpha = np.random.rand(num_users, num_groups)\n",
    "    alpha = alpha / alpha.sum(axis=1).reshape(-1,1)\n",
    "    U = U_subgroups @ alpha.T\n",
    "    # pseudo user points\n",
    "    V = -2*M @ U\n",
    "    # generate items and comparisons\n",
    "    Xdata = np.random.multivariate_normal(np.zeros(feature_dim), (1/feature_dim)*np.eye(feature_dim), num_items).T\n",
    "    S = list(zip(list(np.repeat(list(range(num_users)), num_pairs_per_user)),\n",
    "                       [tuple(np.random.choice(num_items, 2, replace=False)) for _ in range(num_pairs_per_user*num_users)]))\n",
    "    Y, Y_noiseless, Y_unquant = utils.one_bit_pairs(Xdata, S, M, V, noise_type, noise_beta)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# group_dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from group_dataset import Dataset\n",
    "from metric_check import train_main\n",
    "\n",
    "########### hyperparameters for the dataset ################\n",
    "feature_dim = 10\n",
    "metric_rank = 1\n",
    "num_items = 100\n",
    "num_users = 10\n",
    "num_pairs_per_user = 1000\n",
    "noise_type = 'logistic'\n",
    "noise_beta = 1\n",
    "num_groups = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dataset = Dataset(dataset_type='Normal',\n",
    "                         d=feature_dim,\n",
    "                         r=metric_rank,\n",
    "                         n=num_items,\n",
    "                         N=num_users,\n",
    "                         m=num_pairs_per_user,\n",
    "                         noise_type=noise_type,\n",
    "                         noise_beta=noise_beta,\n",
    "                         num_groups=num_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### initialize the dataloders ################\n",
    "normal_data = normal_dataset.getAllData()\n",
    "items, observations, true_y, true_M, true_u = normal_data['X'], normal_data['S'], normal_data['Y'], normal_data['M'], normal_data['U']\n",
    "true_y_noiseless = normal_dataset.Y_noiseless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:32<00:00, 21.51it/s, test_accu=0.804]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "        'feature_dim': 10,\n",
    "        'metric_rank': 1,\n",
    "        'num_items': 100,\n",
    "        'num_users': 10,\n",
    "        'num_pairs_per_user': 300,\n",
    "}\n",
    "\n",
    "train_stats, learner = train_main(args, normal_dataset, relative_error_ind=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [16:04<00:00,  2.07it/s, test_accu=0.818]\n",
      " 11%|█         | 1/9 [16:05<2:08:42, 965.34s/it]"
     ]
    }
   ],
   "source": [
    "from group_dataset import Dataset\n",
    "from metric_check import train_main\n",
    "from tqdm import tqdm\n",
    "\n",
    "res_list = []\n",
    "\n",
    "for i in tqdm(range(1,10)):\n",
    "    ########### hyperparameters for the dataset ################\n",
    "    feature_dim = 10\n",
    "    metric_rank = 10\n",
    "    num_items = 100\n",
    "    num_users = 100\n",
    "    num_pairs_per_user = 1000\n",
    "    noise_type = 'logistic'\n",
    "    noise_beta = 1\n",
    "    num_groups = i\n",
    "\n",
    "    normal_dataset = Dataset(dataset_type='Normal',\n",
    "                            d=feature_dim,\n",
    "                            r=metric_rank,\n",
    "                            n=num_items,\n",
    "                            N=num_users,\n",
    "                            m=num_pairs_per_user,\n",
    "                            noise_type=noise_type,\n",
    "                            noise_beta=noise_beta,\n",
    "                            num_groups=num_groups)\n",
    "\n",
    "    ########### initialize the dataloders ################\n",
    "    normal_data = normal_dataset.getAllData()\n",
    "    items, observations, true_y, true_M, true_u = normal_data['X'], normal_data['S'], normal_data['Y'], normal_data['M'], normal_data['U']\n",
    "    true_y_noiseless = normal_dataset.Y_noiseless\n",
    "\n",
    "    args = {\n",
    "            'feature_dim': feature_dim,\n",
    "            'metric_rank': metric_rank,\n",
    "            'num_items': num_items,\n",
    "            'num_users': num_users,\n",
    "            'num_pairs_per_user': 300,\n",
    "    }\n",
    "\n",
    "    train_stats, learner = train_main(args, normal_dataset, relative_error_ind=False)\n",
    "\n",
    "    res_list.append((train_stats, learner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(res_list,'subgroups_res_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true prediction (just for verification) original just one subgroup version\n",
    "delta_s = []\n",
    "pred_ys = []\n",
    "for obs in S:\n",
    "    user_id, comparison_pair = obs\n",
    "    x_i, x_j = comparison_pair\n",
    "    delta = (Xdata[:,x_i]-U[:,user_id]).T @ M @ (Xdata[:,x_i]-U[:,user_id]) - (Xdata[:,x_j]-U[:,user_id]).T @ M @ (Xdata[:,x_j]-U[:,user_id])\n",
    "    delta_s.append(delta)\n",
    "    if delta > 0:\n",
    "        pred_y = 1\n",
    "    else:\n",
    "        pred_y = -1\n",
    "    pred_ys.append(pred_y)\n",
    "assert np.all(pred_ys == Y_noiseless), \"doesn't match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true prediction (just for verification) original version\n",
    "delta_s = []\n",
    "pred_ys = []\n",
    "for obs in observations:\n",
    "    user_id, comparison_pair = obs\n",
    "    x_i, x_j = comparison_pair\n",
    "    delta = (items[:,x_i]-true_u[:,user_id]).T @ true_M @ (items[:,x_i]-true_u[:,user_id]) - (items[:,x_j]-true_u[:,user_id]).T @ true_M @ (items[:,x_j]-true_u[:,user_id])\n",
    "    delta_s.append(delta)\n",
    "    if delta > 0:\n",
    "        pred_y = 1\n",
    "    else:\n",
    "        pred_y = -1\n",
    "    pred_ys.append(pred_y)\n",
    "assert np.all(pred_ys == true_y_noiseless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
